{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header_01",
   "metadata": {},
   "source": [
    "# Project: Job Interview Guide Workshop\n",
    "**Student:** Lohith Reddy Danda\n",
    "**Group 2 Members:** Ali Cihan Ozdemir, Lohith Reddy Danda\n",
    "\n",
    "**(Note: Roshan did not participate)**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# LLM Interview Session Log\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "--- Prompt\n",
    "You are a seasoned Data Scientist, Machine Learning Engineer, and technical interviewer.\n",
    "I am a Data Scientist and ML Engineer, fresh out of college. You will interview me for an ML Specialist role.\n",
    "1) Unzip and read StudyMaterials.zip. Understand the workshop notebooks it contains. Produce a **500-word summary** of the ML learning content and coding patterns.\n",
    "2) Read StudyGuide.txt. Produce a **100-word summary** of interview topics emphasized.\n",
    "3) **Match** the study guide topics to the workshop materials. Create a **table** listing each topic, whether it is covered by the materials, and any **gaps**.\n",
    "4) Create **15 multiple-choice questions** (A\u2013E) spanning: supervised vs. unsupervised, variables, train/val/test, linear & logistic regression (R\u00b2, MSE, cross-entropy), KNN (hyperparams), decision trees (leaf nodes/predictions), plus **scenario-based/behavioral** items (e.g., imbalanced data, data leakage, model choice trade-offs). Ask **one question at a time**. After I answer all, **score me**.\n",
    "5) Based on questions I get wrong, generate a **new Jupyter Notebook** named **JobInterviewGuide_Workshop.ipynb** inside a folder **JobInterviewGuide_Workshop**. Include:\n",
    "   - Clear **Markdown explanations** of weak topics\n",
    "   - **Python code scaffolding** with exercises and TODOs\n",
    "   - Small, realistic examples and sanity checks\n",
    "   - A short **reflection** prompt about what I learned\n",
    "   - Use the style and structure of the workshop notebooks in the zip as inspiration.\n",
    "Stop here and **wait for my command to start the quiz**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Interviewer (LLM): Summary of ML Learning Content\n",
    "---\n",
    "The workshop notebooks cover a steady progression from basic data handling to more advanced machine learning models. We start with **Data Stream Visualization**, which sets up the foundation for continuous data plotting.\n",
    "\n",
    "Next, we move into parametric models with **Linear Regression**. Here, evaluating the model is just as important as training it. We look at residuals (the errors) to ensure they are homoscedastic (constant variance) and normally distributed. We also evaluate the models using R-squared and Mean Squared Error (MSE).\n",
    "\n",
    "For classification, **Logistic Regression** is our core focus. We interpret log-odds through coefficients and the sigmoid function, and replace MSE with **Cross-Entropy (Log-Loss)** as our primary optimization metric.\n",
    "\n",
    "Moving to non-parametric models, we explore **K-Nearest Neighbors (KNN)**. We test different hyperparameter configurations, looking closely at how the number of neighbors ($k$) and different distance metrics impact model bias and variance.\n",
    "\n",
    "Finally, **Decision Trees** help us understand non-linear, tree-based splits. Visualizing the tree and its leaf nodes allows us to easily explain predictions, while tweaking the maximum depth helps prevent overfitting to noise.\n",
    "\n",
    "Across all these algorithms, rigorous experimental design is key. This means understanding exactly what makes a model **supervised** vs **unsupervised**, and using clean **Train/Validation/Test Splits**. Most importantly, we protect against **Data Leakage** by strictly scaling data inside of a Scikit-Learn `Pipeline`, ensuring test data statistics never bleed into training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Interviewer (LLM): Key Interview Topics\n",
    "---\n",
    "Based on the study materials, this interview will prioritize core ML workflows over specific deep learning architectures. Specifically, we'll verify you can separate supervised from unsupervised problems and handle Train/Validation/Test splits properly. Expect a strong focus on recognizing and preventing training data leakage. You'll also need to demonstrate an understanding of the trade-offs and evaluation metrics for Linear Regression (R\u00b2, MSE), Logistic Regression (Cross-Entropy), KNN (hyperparameter selection), and Decision Trees (leaf operations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Interviewer (LLM): Coverage Table\n",
    "---\n",
    "| Study Guide Topic | Covered? | Notes |\n",
    "| :--- | :---: | :--- |\n",
    "| Supervised vs. Unsupervised | Yes | Core concept across notebooks |\n",
    "| Dependent vs. Independent Variables | Yes | Standard practice |\n",
    "| Train / Validation / Test Split | Yes | Stratification used extensively |\n",
    "| Data Leakage | Yes | Key focus in preprocessing pipelines |\n",
    "| Linear Regression (R\u00b2, MSE) | Yes | Dedicated workshop |\n",
    "| Logistic Regression (Log-Loss) | Yes | Dedicated workshop |\n",
    "| KNN (k, Distance Metrics) | Yes | Dedicated workshop |\n",
    "| Decision Trees | Yes | Interpretation & Metrics |\n",
    "| Trade-offs & Selection | Yes | Model comparisons |\n",
    "| Advanced Imbalanced Data Handling | **Partial Gap** | SMOTE/ADASYN not deeply emphasized |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "Begin the quiz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Great. Let's begin the technical and behavioral assessment.\n**Question 1:** You are building a model to predict whether a given email is 'Spam' or 'Not Spam' based on word frequencies. What type of machine learning problem is this?\n- **A)** Regression\n- **B)** Unsupervised Learning\n- **C)** Classification\n- **D)** Reinforcement Learning\n- **E)** Semi-supervised Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Correct.\n",
    "**Question 2:** You have a massive log of website visitor behavior without any predefined categories or labels. You want to group users who exhibit similar browsing patterns. Which approach is most appropriate?\n- **A)** Supervised Classification\n- **B)** Unsupervised Clustering\n- **C)** Supervised Regression\n- **D)** Transfer Learning\n- **E)** Active Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Correct.\n",
    "**Question 3:** Why is it critical to keep the 'Test' set completely separate and unseen until the end of the entire modeling process?\n- **A)** To have extra data in case the model is too slow to train.\n- **B)** To check for missing values accurately.\n- **C)** To tune hyperparameters during training.\n- **D)** To provide a strictly unbiased estimate of the final model's performance on fully unseen real-world data.\n- **E)** To ensure data leakage occurs smoothly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Correct. \n",
    "**Question 4 (Scenario):** You are evaluating a classification model for credit card fraud where 99.9% of transactions are legitimate and 0.1% are fraudulent. Why is 'Accuracy' a misleading metric here?\n- **A)** Because accuracy is only used for regression.\n- **B)** Because standard accuracy doesn't handle numerical values.\n- **C)** Because the model can simply predict 'legitimate' every time and still achieve 99.9% accuracy without actually finding fraud.\n- **D)** Because accurate models are slower.\n- **E)** Because accuracy requires a 50/50 split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Correct.\n",
    "**Question 5:** You have missing values in your dataset. You use a `SimpleImputer` to fill the missing values with the mean of the corresponding feature across the *entire* dataset before splitting it into train and validation sets. What is the main issue with this approach?\n- **A)** It takes too long to calculate the mean.\n- **B)** You are causing Data Leakage because the mean calculation includes information from the validation set.\n- **C)** It reduces variance too much.\n- **D)** It only works for K-Nearest Neighbors.\n- **E)** Mean imputation is generally deprecated in Scikit-Learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Incorrect. Let's move on, we will revisit this at the end.\n",
    "**Question 6:** In Linear Regression, evaluating performance using Mean Squared Error (MSE) is common. What does MSE actually represent mathematically?\n- **A)** The percentage of variance explained by the model.\n- **B)** The absolute differences between predictions and actuals.\n- **C)** The average of the squared differences between the predicted values and the actual target values.\n- **D)** The squared sum of all features.\n- **E)** The root of the R-squared value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Correct.\n",
    "**Question 7:** Linear Regression assumes that your independent variables are not highly correlated with each other. If they are strongly correlated, creating unstable coefficients, what is this issue called?\n- **A)** Heteroscedasticity\n- **B)** Non-linearity\n- **C)** Autocorrelation\n- **D)** Multicollinearity\n- **E)** Endogeneity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Correct.\n",
    "**Question 8:** Logistic regression maps raw output values into probabilities using the sigmoid (logistic) function. What is the specific output range of the sigmoid function?\n- **A)** [-1, 1]\n- **B)** [-infinity, 0]\n- **C)** [0, 1]\n- **D)** [0, 100]\n- **E)** [0, infinity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Correct.\n",
    "**Question 9:** For evaluating a Logistic Regression model, what happens to the Log-Loss (Cross-Entropy) metric as the predicted probability of the *true* class gets closer and closer to 1.0 (perfect certainty)?\n- **A)** It approaches 1.0\n- **B)** It goes to negative infinity\n- **C)** It stays constant at 0.5\n- **D)** It approaches 0.0\n- **E)** It approaches infinity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Correct.\n",
    "**Question 10:** In a Decision Tree algorithm applied to classification, if a node contains samples belonging entirely to one single class, what is its Gini impurity score?\n- **A)** 1.0\n- **B)** 0.5\n- **C)** 0.0\n- **D)** -1.0\n- **E)** Infinity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Correct.\n",
    "**Question 11:** How does the hyperparameter `min_samples_leaf` help prevent a Decision Tree from overfitting?\n- **A)** It ensures trees don't create splits that result in leaf nodes containing very few samples, thus smoothing the model and reducing variance.\n- **B)** It forces the tree to have exactly that many leaves.\n- **C)** It limits the maximum depth of the tree directly.\n- **D)** It increases bias to infinity.\n- **E)** It removes the need for training entirely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Correct.\n",
    "**Question 12:** To choose the optimal hyperparameter 'k' in K-Nearest Neighbors, what is generally the best approach?\n- **A)** Set k equal to the number of features.\n- **B)** Always use k=1.\n- **C)** Set k equal to the total number of samples (N).\n- **D)** Randomly pick k between 1 and 100.\n- **E)** Evaluate different values of k using cross-validation on the training set and choose the one that minimizes validation error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Correct.\n",
    "**Question 13:** While Euclidean distance is standard for KNN with continuous data, which distance metric might be better suited for calculating similarity between highly sparse, high-dimensional textual data representations?\n- **A)** Haversine distance\n- **B)** Manhattan distance\n- **C)** Cosine similarity / distance\n- **D)** Chebyshev distance\n- **E)** Mahalanobis distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Correct.\n",
    "**Question 14 (Scenario):** You are using KNN to predict loan default. A feature like 'Income' ranges from 20,000 to 150,000, while 'Age' ranges from 18 to 80. If you do NOT scale the features, what will happen to the distance calculations?\n- **A)** Unscaled variables throw an error in Scikit-Learn.\n- **B)** Age will completely dominate the distance calculation.\n- **C)** The Income variable's massive numerical range will dominate the distance metric, rendering the Age variable almost entirely irrelevant.\n- **D)** The distance calculation reverses itself.\n- **E)** Missing values will be automatically generated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Correct.\n",
    "**Question 15 (Scenario/Behavioral):** For a critical medical diagnostics tool, doctors require the ability to understand step-by-step *how* a prediction was determined. They are willing to sacrifice 1-2% accuracy for this transparency. Which model is heavily favored in this case?\n- **A)** A 100-layer Deep Neural Network\n- **B)** Decision Tree (or a simple Logistic Regression model)\n- **C)** Extreme Gradient Boosting (XGBoost)\n- **D)** Random Forest with 1000 estimators\n- **E)** Support Vector Machine with a non-linear RBF kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---: Final Evaluation\n",
    "**Final Score: 14/15 (93%)**\n",
    "Excellent work! You demonstrated a very strong grasp of foundational machine learning concepts, evaluation metrics, and critical model trade-offs. Your reasoning on the scenario-based questions was impressive.\n",
    "**Areas for Improvement:**\n",
    "You missed exactly one question regarding **Data Leakage during Imputation** (Question 5). You incorrectly stated that mean imputation is deprecated. The actual issue is **Data Leakage**. By fitting `SimpleImputer` on the *entire* dataset before splitting it, the mean calculation includes values from the validation set. This \"leaks\" information from the unseen data into your training process, artificially inflating the model's performance metrics. To properly handle this, you need to restrict the `fit` method of the imputer exclusively to the training subset, ideally chaining it inside a Scikit-Learn `Pipeline`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "follow_up_q",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "**Follow-Up Challenge to address your Knowledge Gap:**\n",
    "\n",
    "Let's reinforce the concept of avoiding Data Leakage during imputation. \n",
    "\n",
    "Can you write a brief Python snippet using Scikit-Learn's `Pipeline` to safely chain a `SimpleImputer` (using the mean strategy) and a `LogisticRegression` model, ensuring the imputer is only fit on the training data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "follow_up_a",
   "metadata": {},
   "source": [
    "## Candidate (You)\n",
    "---\n",
    "Certainly. Wrapping the imputer in a Pipeline ensures data isn't leaked:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit strictly on the training data!\n",
    "pipe.fit(X_train, y_train)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "follow_up_eval",
   "metadata": {},
   "source": [
    "## Interviewer (LLM)\n",
    "---\n",
    "Perfect! That is entirely correct. \n",
    "\n",
    "By using a `Pipeline`, `fit` is only called on the training folds during cross-validation, and the imputer only `transform`s the validation data using the training mean. This prevents data leakage completely.\n",
    "\n",
    "I have now fully verified your understanding and your knowledge gap is officially closed. Fantastic job on this interview!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}